{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте при помощи pyTorch функцию, которая возвращает сумму (x.sum()) элементов тензора X, строго превышающих значение limit, которое является входным значением алгоритма.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(44)\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "limit = int(input())\n",
    "\n",
    "larger_than_limit_sum = X[X > limit].sum()\n",
    "\n",
    "print(larger_than_limit_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте расчет градиента для функции \n",
    "$ f(w) = \\prod_{i,j}\\ln\\ln(w_{i,j}+7) $ в точке $ w = [[5,10],[1,2]] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0201, 0.0109],\n",
      "        [0.0449, 0.0351]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[5.,10.],[1.,2.]], requires_grad=True)\n",
    "\n",
    "function = (w + 7).log().log().prod()\n",
    "function.backward()\n",
    "\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте градиентный спуск для той же функции $ f(w) = \\prod_{i,j}\\ln\\ln(w_{i,j}+7) $ в точке $ w = [[5,10],[1,2]] $\n",
    "\n",
    "шаг градиентного спуска $ \\alpha=0.001 $\n",
    "\n",
    "Чему будет равен $w^{t=500}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9900, 9.9948],\n",
      "        [0.9775, 1.9825]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# x.data - tensor with requires_grad=False\n",
    "# gradients are summing -> x.grad.zero_() (inplace)\n",
    "\n",
    "w = torch.tensor([[5., 10.], [1., 2.]], requires_grad=True)\n",
    "alpha = 0.001\n",
    "\n",
    "for _ in range(500):\n",
    "    # it's critical to calculate function inside the loop:\n",
    "    function = (w + 7).log().log().prod()\n",
    "    function.backward()\n",
    "    w.data -=  alpha * w.grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите пример, используя torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9900, 9.9948],\n",
      "        [0.9775, 1.9825]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# optimizer.step() performes gradient descend step\n",
    "\n",
    "w = torch.tensor([[5., 10.], [1., 2.]], requires_grad=True)\n",
    "alpha = 0.001\n",
    "optimizer =  torch.optim.SGD([w], lr=alpha)\n",
    "\n",
    "for _ in range(500):\n",
    "    # it's critical to calculate function inside the loop:\n",
    "    function = (w + 7).log().log().prod()\n",
    "    function.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попрактикуемся с SineNet:\n",
    "\n",
    "1) Добавим еще один fc-слой\n",
    "\n",
    "2) Заменим активацию между слоями на гиперболический тангенс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "SineNet(\n",
      "  (fc1): Linear(in_features=1, out_features=20, bias=True)\n",
      "  (act1): Tanh()\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (act2): Tanh()\n",
      "  (fc3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SineNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Tanh()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n",
    "        self.act2 = torch.nn.Tanh()\n",
    "        self.fc3 = torch.nn.Linear(n_hidden_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "sine_net = SineNet(int(input()))\n",
    "sine_net.forward(torch.Tensor([1.]))\n",
    "\n",
    "print(sine_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть для задачи регрессии:\n",
    "\n",
    "Возьмем более сложную функцию в качестве таргета: \n",
    "\n",
    "$$ y = 2^x\\sin(2^{−x}) $$\n",
    "\n",
    "Кроме того, мы хотим получить хорошую метрику MAE на валидации: \n",
    "\n",
    "$$ MAE = \\frac{1}{l} \\sum_{i=1}^{l}\\lvert y\\_pred_i - y\\_target_i \\rvert $$\n",
    "\n",
    "тогда как знакомая нам MSE выглядит как \n",
    "\n",
    "$$ MSE = \\frac{1}{l} \\sum_{i=1}^{l}(y\\_pred_i - y\\_target_i)^2 $$\n",
    "\n",
    "Получите метрику не хуже 0.03\n",
    "\n",
    "Что можно варьировать: \n",
    "\n",
    "1) Архитектуру сети\n",
    "\n",
    "2) loss-функцию\n",
    "\n",
    "3) lr оптимизатора\n",
    "\n",
    "4) Количество эпох в обучении\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017926551401615143\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_epoches = 1500\n",
    "n_hidden = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "def target_function(x):\n",
    "    return 2**x * torch.sin(2**-x)\n",
    "\n",
    "# для создания нейронной сети нужно отнаследоваться от torch.nn.Module\n",
    "class RegressionNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = RegressionNet(n_hidden)\n",
    "\n",
    "# ------Dataset preparation start--------:\n",
    "x_train =  torch.linspace(-10, 5, 100)\n",
    "y_train = target_function(x_train)\n",
    "noise = torch.randn(y_train.shape) / 20.\n",
    "y_train = y_train + noise\n",
    "\n",
    "# unsqueeze_ -> строка превращается в столбец\n",
    "x_train.unsqueeze_(1)\n",
    "y_train.unsqueeze_(1)\n",
    "\n",
    "x_validation = torch.linspace(-10, 5, 100)\n",
    "y_validation = target_function(x_validation)\n",
    "x_validation.unsqueeze_(1)\n",
    "y_validation.unsqueeze_(1)\n",
    "# ------Dataset preparation end--------:\n",
    "\n",
    "# net.paramerers() - веса сети, которые надо передать в оптимизатор, чтобы он их менял\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "def loss(pred, target):\n",
    "    return ((pred - target) ** 2).mean()\n",
    "\n",
    "# Эпоха - итерация обучения на всем датасете\n",
    "for epoch_index in range(num_epoches):\n",
    "    optimizer.zero_grad() # 1 - зануляем градиенты (для библиотеки PyTorch, иначе градиенты складываются)\n",
    "\n",
    "    y_pred = net.forward(x_train) # 2 - вычисляем результат\n",
    "    loss_value = loss(y_pred, y_train) # 3 - считаем лосс, который помнит, как он был посчитан с помощью весов\n",
    "    loss_value.backward() # 4 - считаем градиенты для лосса\n",
    "    optimizer.step() # 5 - пересчитываем веса на основе вычисленных градиентов для уменьшения лосса\n",
    "\n",
    "def metric(pred, target):\n",
    "    return (pred - target).abs().mean()\n",
    "\n",
    "print(metric(net.forward(x_validation), y_validation).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(net, x, y):\n",
    "    y_pred = net.forward(x)\n",
    "\n",
    "    plt.plot(x.numpy(), y.numpy(), 'o', label='Groud truth')\n",
    "    plt.plot(x.numpy(), y_pred.data.numpy(), '-', c='r', label='Prediction');\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "\n",
    "\n",
    "predict(net, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фиксирование seed не гарантирует, что код будет одинаково выполняться на разных компьютерах. Но на одной и той же машине вы будете получать одинаковые результаты, перезапуская один и тот же скрипт.\n",
    "\n",
    "Например, функция random.randint(start, end) отдает случайное целое число в диапазоне от start, end (включительно). Запуская скрипт, состоящий из вызова этой функции, вы будете получать разные ответы. \n",
    "\n",
    "В этом задании вам нужно подобрать seed, чтобы функция random.randint(0, 10) выдала число 5\n",
    "\n",
    "PS: надо понимать, что фиксирование random seed не приведет к тому, что повторный вызов random.randint(0, 10) внутри того же скрипта снова даст 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(7)\n",
    "print(random.randint(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было сказано в предыдущем уроке, полносвязный слой может быть представлен как матричное умножение матрицы входов (X) и матрицы весов нейронов слоя (W), плюс вектор bias'ов слоя (b). \n",
    "\n",
    "В документации к классу torch.nn.Linear (полносвязному слою) написано следующее: Applies a linear transformation to the incoming data: \n",
    "$$ y=xA^T+b $$ $А$ здесь – это то, как PyTorch хранит веса слоя. Но чтобы эта матрица совпала с W из предыдущего урока, нужно её сперва транспонировать.\n",
    "\n",
    "Давайте реализуем функциональность torch.nn.Linear и сверим с оригиналом!\n",
    "\n",
    "Пусть у нас будет 1 объект x на входе с двумя компонентами. Его мы передадим в полносвязный слой с 3-мя нейронами и получим, соотсветственно, 3 выхода. После напишем эту же функциональность с помощью матричного умножения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Сперва создадим тензор x:\n",
    "x = torch.tensor([[10., 20.]])\n",
    "\n",
    "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
    "fc = torch.nn.Linear(2, 3)\n",
    "\n",
    "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
    "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
    "\n",
    "# Давайте проставим свои значения в веса и bias'ы:\n",
    "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
    "fc.weight.data = w\n",
    "\n",
    "b = torch.tensor([[31., 32., 33.]])\n",
    "fc.bias.data = b\n",
    "\n",
    "# Получим выход fc-слоя:\n",
    "fc_out = fc(x)\n",
    "\n",
    "# Попробуем теперь получить аналогичные выходы с помощью матричного перемножения:\n",
    "# fc_out_alternative = x @ w.T + b\n",
    "\n",
    "fc_out_alternative = x @ w.transpose(-1, -2) + b\n",
    "\n",
    "# Проверка осуществляется автоматически вызовом функции\n",
    "print(fc_out == fc_out_alternative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем шаге мы написали функцию, эмулирующую fc-слой. Проверим, что по ней правильно считается градиент. \n",
    "\n",
    "Функцию backward() в PyTorch можно посчитать только от скалярной функции (выход из такой функции – одно число). Это логично, так как loss-функция выдает всегда одно число. Но fc-слой, который мы проэмулировали, имел 3 выхода. Предлагаем их просуммировать, чтобы получить в итоге скалярную функцию. Заметим, впрочем, что можно было бы выбрать любую агрегирующую операцию, например умножение.\n",
    "\n",
    "Дополните код так, чтобы градиент по весам и смещениям (bias) совпадал с аналогичным градиентом в вашей фунции.\n",
    "\n",
    "Чем обусловлен полученный градиент? Изменится ли он, если мы подадим другие входы или другую инициализацию весов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_weight_grad: tensor([[10., 20.],\n",
      "        [10., 20.],\n",
      "        [10., 20.]])\n",
      "our_weight_grad: tensor([[10., 20.],\n",
      "        [10., 20.],\n",
      "        [10., 20.]])\n",
      "fc_bias_grad: tensor([[1., 1., 1.]])\n",
      "out_bias_grad: tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Сперва создадим тензор x:\n",
    "x = torch.tensor([[10., 20.]])\n",
    "\n",
    "# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\n",
    "fc = torch.nn.Linear(2, 3)\n",
    "\n",
    "# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n",
    "# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n",
    "\n",
    "# Давайте проставим свои значения в веса и bias'ы:\n",
    "w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\n",
    "fc.weight.data = w\n",
    "\n",
    "b = torch.tensor([[31., 32., 33.]])\n",
    "fc.bias.data = b\n",
    "\n",
    "# Получим выход fc-слоя:\n",
    "fc_out = fc(x)\n",
    "# Просуммируем выход fc-слоя, чтобы получить скаляр:\n",
    "fc_out_summed = fc_out.sum()\n",
    "\n",
    "# Посчитаем градиенты формулы fc_out_summed:\n",
    "fc_out_summed.backward()\n",
    "weight_grad = fc.weight.grad\n",
    "bias_grad = fc.bias.grad\n",
    "\n",
    "# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\n",
    "# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "# Получим выход нашей формулы:\n",
    "our_formula = (x @ w.transpose(-1, -2) + b).sum() # SUM{x * w^T + b}\n",
    "\n",
    "# Сделайте backward для нашей формулы:\n",
    "our_formula.backward()\n",
    "\n",
    "# Проверка осуществляется автоматически, вызовом функций:\n",
    "print('fc_weight_grad:', weight_grad)\n",
    "print('our_weight_grad:', w.grad)\n",
    "print('fc_bias_grad:', bias_grad)\n",
    "print('out_bias_grad:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом уроке мы с вами реализуем прямой проход сверточного слоя, обратный проход и расчет производных мы трогать не будем.\n",
    "\n",
    "Вспомним как работает сверточный слой:\n",
    "\n",
    "1) на вход подается массив изображений, еще он называется батчем\n",
    "\n",
    "2) к каждому изображению по границам добавляются нули\n",
    "\n",
    "3) по каждому изображению \"скользит\" каждый из фильтров сверточного слоя\n",
    "\n",
    "Давайте начнем с разминки - реализуем функцию, добавляющую padding.\n",
    "\n",
    "Пусть у нас есть батч input_images из двух изображений с тремя каналами (RGB). Размер изображений пусть будет 3*3. Вспомним, что вход сверточного слоя имеет следующую размерность:\n",
    "\n",
    "* размер батча\n",
    "\n",
    "* число каналов\n",
    "\n",
    "* высота\n",
    "\n",
    "* ширина\n",
    "\n",
    "В рассматриваемом случае размерность входа (2, 3, 3, 3).\n",
    "\n",
    "Если мы добавим вокруг каждого изображения отступ из одного нуля, то размер каждого изображений станет 3+2*1 = 5 пикселей в ширину и 5 в высоту соответственно (добавляем по одному нулю с каждой стороны изображения).\n",
    "\n",
    "Напишите любую работающую реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Создаем входной массив из двух изображений RGB 3*3\n",
    "input_images = torch.tensor(\n",
    "      [[[[0,  1,  2],\n",
    "         [3,  4,  5],\n",
    "         [6,  7,  8]],\n",
    "\n",
    "        [[9, 10, 11],\n",
    "         [12, 13, 14],\n",
    "         [15, 16, 17]],\n",
    "\n",
    "        [[18, 19, 20],\n",
    "         [21, 22, 23],\n",
    "         [24, 25, 26]]],\n",
    "\n",
    "\n",
    "       [[[27, 28, 29],\n",
    "         [30, 31, 32],\n",
    "         [33, 34, 35]],\n",
    "\n",
    "        [[36, 37, 38],\n",
    "         [39, 40, 41],\n",
    "         [42, 43, 44]],\n",
    "\n",
    "        [[45, 46, 47],\n",
    "         [48, 49, 50],\n",
    "         [51, 52, 53]]]])\n",
    "\n",
    "\n",
    "def get_padding2d(input_images):\n",
    "    # добавить нулей с четырех сторон каждого изображения\n",
    "    shape = input_images.shape\n",
    "    padded_images = torch.zeros((shape[0],shape[1],shape[2]+2,shape[3]+2))\n",
    "    for i in range(shape[0]):\n",
    "        padded_images[i][:,1:-1,1:-1] = input_images[i]\n",
    "    return padded_images.float()\n",
    "\n",
    "\n",
    "correct_padded_images = torch.tensor(\n",
    "       [[[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  0.,  1.,  2.,  0.],\n",
    "          [0.,  3.,  4.,  5.,  0.],\n",
    "          [0.,  6.,  7.,  8.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  9., 10., 11.,  0.],\n",
    "          [0., 12., 13., 14.,  0.],\n",
    "          [0., 15., 16., 17.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 18., 19., 20.,  0.],\n",
    "          [0., 21., 22., 23.,  0.],\n",
    "          [0., 24., 25., 26.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]],\n",
    "\n",
    "\n",
    "        [[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 27., 28., 29.,  0.],\n",
    "          [0., 30., 31., 32.,  0.],\n",
    "          [0., 33., 34., 35.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 36., 37., 38.,  0.],\n",
    "          [0., 39., 40., 41.,  0.],\n",
    "          [0., 42., 43., 44.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 45., 46., 47.,  0.],\n",
    "          [0., 48., 49., 50.,  0.],\n",
    "          [0., 51., 52., 53.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]]])\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "print(torch.allclose(get_padding2d(input_images), correct_padded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2,3,5,5))\n",
    "b = torch.ones((2,3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[2], a.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    print(a[i][:,1:-1,1:-1])\n",
    "    a[i][:,1:-1,1:-1] = b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом шаге детально рассмотрим из чего состоит сверточный слой.\n",
    "\n",
    "Сверточный слой это массив фильтров.\n",
    "\n",
    "Каждый фильтр имеет следующую размерность:\n",
    "\n",
    "* число слоев во входном изображении (для RGB это 3)\n",
    "\n",
    "* высота фильтра\n",
    "\n",
    "* ширина фильтра\n",
    "\n",
    "В ядре (кернеле) все фильтры имеют одинаковые размерность, поэтому ширину и высоту фильтров называют шириной и высотой ядра. Чаще всего ширина ядра равна высоте ядра, в таком случае их называют размером ядра (kernel_size).\n",
    "\n",
    "Также слой имеет такие параметры:\n",
    "\n",
    "* padding - на какое количество пикселей увеличивать входное изображение с каждой стороны.\n",
    "\n",
    "* stride - на сколько пикселей смещается фильтр при вычислении свертки\n",
    "\n",
    " \n",
    "Попробуйте самостоятельно вывести формулу размерности выхода сверточного слоя, зная параметры входа и ядра. \n",
    "\n",
    "Правильность формулы проверьте, сравнив ее с формулой из документации.\n",
    "\n",
    "\n",
    "Чтобы убедиться в правильности вашей формулы, напишите функцию, принимающую на вход:\n",
    "\n",
    "* входную размерность (число изображений в батче * число слоев в одном изображении * высота изображения * ширина изображения)\n",
    "\n",
    "* количество фильтров\n",
    "\n",
    "* размер фильтров (считаем, что высота совпадает с шириной)\n",
    "\n",
    "* padding\n",
    "\n",
    "* stride\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[ 2 10  8  8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    out_shape = np.full(4, 0)\n",
    "    out_shape[0] = input_matrix_shape[0]\n",
    "    out_shape[1] = out_channels\n",
    "    out_shape[2] = np.floor((input_matrix_shape[2] + 2 * padding - (kernel_size - 1) - 1) / stride + 1)\n",
    "    out_shape[3] = np.floor((input_matrix_shape[3] + 2 * padding - (kernel_size - 1) - 1) / stride + 1)\n",
    "\n",
    "    return out_shape\n",
    "\n",
    "print(np.array_equal(\n",
    "    calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n",
    "                   out_channels=10,\n",
    "                   kernel_size=3,\n",
    "                   stride=1,\n",
    "                   padding=0),\n",
    "    [2, 10, 8, 8]))\n",
    "\n",
    "print(calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n",
    "                   out_channels=10,\n",
    "                   kernel_size=3,\n",
    "                   stride=1,\n",
    "                   padding=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переиспользуем код с предыдущего шага для проверки своей реализации сверточного слоя.\n",
    "\n",
    "Рассмотрим свертку батча из одного однослойного изображения 3 * 3 с ядром из одного фильтра 2 * 2, stride = 1, то есть, на выходе должна получиться одна матрица 2 * 2. Строго записанная размерность выхода равна (1 - изображений в батче, 1 - количество фильтров в ядре, 2 - высота матрицы выхода, 2 - ширина матрицы выхода)\n",
    "\n",
    "<img src=\"img1.png\" width=50%/>\n",
    "\n",
    "На каждой итерации цикла фильтр умножается попиксельно на часть изображения, а потом 4 получившиеся числа складываются - получается один пиксель выхода.\n",
    "\n",
    "Требуемое количество итераций для данного случая - 4, так как может быть 2 положения ядра и 2 по вертикали, общее число итераций - произведение количеств положений, то есть в данном случае 2 * 2 = 4.\n",
    "\n",
    "Давайте перейдем от простого случая к общему.\n",
    "\n",
    "Если бы изображение было многослойным, например трехслойное - RGB, значит, фильтры в ядре тоже должны быть трехслойные. Каждый слой фильтра попиксельно умножается на соответствующий слой исходного изображения. То есть в данном случае после умножения получилось бы 4 * 3 = 12 произведений, результаты которых складываются, и получается значение выходного пикселя.\n",
    "\n",
    "Если бы фильтров в ядре было больше одного, то добавился бы внешний цикл по фильтрам, внутри которого мы считаем свертку для каждого фильтра.\n",
    "\n",
    "Если бы во входном батче было более 1 изображения, то добавился бы еще один внешний цикл по изображениям в батче.\n",
    "\n",
    "Напоминание: во всех шагах этого урока мы считаем bias в сверточных слоях нулевым.\n",
    "\n",
    "На этом шаге требуется реализовать сверточный слой через циклы.\n",
    "\n",
    "Обратите внимание, что в коде рассматривается общий случай - батч на входе не обязательно состоит из одного изображения, в ядре несколько слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 5252.]]],\n",
      "\n",
      "\n",
      "        [[[12596.]]]], grad_fn=<CopySlices>) tensor([[[[ 5252.]]],\n",
      "\n",
      "\n",
      "        [[[12596.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
    "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "    return batch_size, out_channels, output_height, output_width\n",
    "\n",
    "\n",
    "class ABCConv2d(ABC):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.kernel = kernel\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, input_tensor):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Conv2d(ABCConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding=0, bias=False)\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.conv2d.weight.data = kernel\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return self.conv2d(input_tensor)\n",
    "\n",
    "\n",
    "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
    "    out_channels = kernel.shape[0]\n",
    "    in_channels = kernel.shape[1]\n",
    "    kernel_size = kernel.shape[2]\n",
    "\n",
    "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
    "    layer.set_kernel(kernel)\n",
    "\n",
    "    return layer(input_matrix)\n",
    "\n",
    "\n",
    "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
    "                      input_height=4, input_width=4, stride=2):\n",
    "    kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "\n",
    "    in_channels = kernel.shape[1]\n",
    "\n",
    "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
    "                                input_height * input_width,\n",
    "                                out=torch.FloatTensor()) \\\n",
    "        .reshape(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
    "        conv2d_layer_class, stride, kernel, input_tensor)\n",
    "    conv2d_out = create_and_call_conv2d_layer(\n",
    "        Conv2d, stride, kernel, input_tensor)\n",
    "    print(custom_conv2d_out, conv2d_out)\n",
    "\n",
    "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
    "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
    "\n",
    "\n",
    "# Сверточный слой через циклы.\n",
    "class Conv2dLoop(ABCConv2d):\n",
    "    def __call__(self, input_tensor):\n",
    "        batch_size, out_channels, output_height, output_width = calc_out_shape(\n",
    "        input_tensor.shape, self.out_channels, self.kernel_size, self.stride, 0)\n",
    "        output_tensor = torch.zeros((batch_size, out_channels, output_height, output_width), requires_grad=True)\n",
    "        for batch_num in range(batch_size):\n",
    "            for out_channel in range(out_channels):\n",
    "                for in_channel in range(self.in_channels):\n",
    "                    for width in range(output_width):\n",
    "                        for height in range(output_height):\n",
    "                            window = input_tensor[batch_num][:,height*self.stride:height*self.stride+self.kernel_size,\n",
    "                                                                         width*self.stride:width*self.stride+self.kernel_size]\n",
    "                            output_tensor[batch_num, out_channel, height, width] = (window * self.kernel[out_channel]).sum()\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "# Корректность реализации определится в сравнии со стандартным слоем из pytorch.\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "\n",
    "print(test_conv2d_layer(Conv2dLoop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переиспользуем код с третьего шага для проверки своей реализации сверточного слоя.\n",
    "\n",
    "Реализация через циклы очень неэффективна по производительности. Есть целых два способа сделать то же самое с помощью матричного умножения. \n",
    "\n",
    "На этом шаге будет реализация первым из них.\n",
    "\n",
    " \n",
    "\n",
    "Рассмотрим свертку одного одноканального изображения размером 4 * 4 пикселя (значения пикселей обозначены через X).\n",
    "\n",
    "Сворачивать будем с ядром из одного фильтра размером 3 * 3, веса обозначены через W.\n",
    "\n",
    "Для простоты примем stride = 1.\n",
    "\n",
    "Тогда выход Y будет иметь размерность 1 * 1 * 2 * 2 (в данном случае на входе одно изображение - это первая единица в размерности, в ядре один фильтр - это вторая единица в размерности выхода).\n",
    "\n",
    "<img src=\"img2.png\"/>\n",
    "\n",
    "Оказывается, выход свертки можно получить умножением матриц, как показано ниже.\n",
    "\n",
    "<img src=\"img3.svg\"/>\n",
    "\n",
    "Рекомендуем убедиться в этом, перемножив матрицы на листочке.\n",
    "\n",
    "Давайте перейдем от простого случая к общему:\n",
    "\n",
    "* Если фильтров в ядре больше одного. Заметим, что для каждого фильтра, матрица W’ будет умножаться на один и тот же вектор изображения. Значит, можно сконкатенировать матрицы фильтров ядра по вертикали и за одно умножение получить ответ для всех фильтров.\n",
    "\n",
    "<img src=\"img4.svg\"/>\n",
    "\n",
    "* Если на входе более одного изображения: заметим, что матрица W’ одинакова для всех изображений батча, то есть, можно каждое изображение вначале вытянуть в столбец, а затем эти столбцы для всех изображений батча сконкатенировать по горизонтали.\n",
    "\n",
    "<img src=\"img5.svg\"/>\n",
    "\n",
    "* Если в изображении больше одного слоя, вначале выполним преобразования входа и ядра для каждого слоя, а затем сконкатенируем: вектора разных слоев входа в один большой вектор, а матрицы ядра соответственно в одну длинную матрицу. И мы получим сложение от выходов по слоям в процессе перемножения матриц.\n",
    "\n",
    "<img src=\"img6.png\"/>\n",
    "\n",
    "То есть даже в самом общем случае мы за одно умножение матриц можем получить ответ.\n",
    "\n",
    "Но рассчитанный таким способом выход не совпадает по размерности с выходом стандартного слоя из PyTorch - нужно изменить размерность.\n",
    "\n",
    " \n",
    "\n",
    "В коде уже реализовано:\n",
    "\n",
    "* преобразование входного батча изображений\n",
    "\n",
    "* умножение матрицы ядра на матрицу входа\n",
    "\n",
    "* преобразование ответа\n",
    "\n",
    "Напоминание: во всех шагах этого урока мы считаем bias в сверточных слоях нулевым.\n",
    "\n",
    "Вам осталось реализовать преобразование ядра в описанный выше формат.\n",
    "\n",
    "Обратите внимание, что в коде рассматривается общий случай - вход состоит из нескольких многослойных изображений, в ядре несколько слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'int' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-06261785d4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Проверка происходит автоматически вызовом следующего кода\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_conv2d_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2dMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-06261785d4a0>\u001b[0m in \u001b[0;36mtest_conv2d_layer\u001b[0;34m(conv2d_layer_class, batch_size, input_height, input_width, stride)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     custom_conv2d_out = create_and_call_conv2d_layer(\n\u001b[0;32m---> 24\u001b[0;31m         conv2d_layer_class, stride, kernel, input_tensor)\n\u001b[0m\u001b[1;32m     25\u001b[0m     conv2d_out = create_and_call_conv2d_layer(\n\u001b[1;32m     26\u001b[0m         Conv2d, stride, kernel, input_tensor)\n",
      "\u001b[0;32m<ipython-input-1-0f4a708c5303>\u001b[0m in \u001b[0;36mcreate_and_call_conv2d_layer\u001b[0;34m(conv2d_layer_class, stride, kernel, input_matrix)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-06261785d4a0>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, torch_input)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mkernel_unsqueezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unsqueeze_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_unsqueezed\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtorch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         return result.permute(1, 0).view((batch_size, self.out_channels,\n\u001b[1;32m     50\u001b[0m                                           output_height, output_width))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'int' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
    "                      input_height=4, input_width=4, stride=2):\n",
    "    kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "\n",
    "    in_channels = kernel.shape[1]\n",
    "\n",
    "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
    "                                input_height * input_width,\n",
    "                                out=torch.FloatTensor()) \\\n",
    "        .reshape(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
    "        conv2d_layer_class, stride, kernel, input_tensor)\n",
    "    conv2d_out = create_and_call_conv2d_layer(\n",
    "        Conv2d, stride, kernel, input_tensor)\n",
    "\n",
    "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
    "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
    "\n",
    "\n",
    "class Conv2dMatrix(ABCConv2d):\n",
    "    # Функция преобразование кернела в матрицу нужного вида.\n",
    "    def _unsqueeze_kernel(self, torch_input, output_height, output_width):\n",
    "        kernel_unsqueezed = 0\n",
    "        return kernel_unsqueezed\n",
    "\n",
    "    def __call__(self, torch_input):\n",
    "        batch_size, out_channels, output_height, output_width\\\n",
    "            = calc_out_shape(\n",
    "                input_matrix_shape=torch_input.shape,\n",
    "                out_channels=self.kernel.shape[0],\n",
    "                kernel_size=self.kernel.shape[2],\n",
    "                stride=self.stride,\n",
    "                padding=0)\n",
    "        print(output_height, output_width)\n",
    "        kernel_unsqueezed = self._unsqueeze_kernel(torch_input, output_height, output_width)\n",
    "        result = kernel_unsqueezed @ torch_input.view((batch_size, -1)).permute(1, 0)\n",
    "        return result.permute(1, 0).view((batch_size, self.out_channels,\n",
    "                                          output_height, output_width))\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "print(test_conv2d_layer(Conv2dMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  0.,  1.,  2.,  1.,  0.,  1.,  0.,  1.,  2.,  1.,  0.,  3.,\n",
       "         3.,  0.,  1., 10., 10., 11., 12., 13., 14., 15., 16., 17., 18.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "k.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  0.],\n",
       "          [ 1.,  2.,  1.],\n",
       "          [ 0.,  1.,  0.]],\n",
       "\n",
       "         [[ 1.,  2.,  1.],\n",
       "          [ 0.,  3.,  3.],\n",
       "          [ 0.,  1., 10.]],\n",
       "\n",
       "         [[10., 11., 12.],\n",
       "          [13., 14., 15.],\n",
       "          [16., 17., 18.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(k, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 8, 16])\n",
      "torch.Size([4, 10, 8, 16])\n",
      "torch.Size([4, 10, 8, 16])\n",
      "torch.Size([4, 10, 8, 16])\n",
      "torch.Size([4, 10, 8, 16])\n",
      "torch.Size([4, 10, 22, 30])\n",
      "torch.Size([4, 10, 7, 15])\n",
      "torch.Size([4, 10, 9, 17])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N = 4\n",
    "C = 3\n",
    "C_out = 10\n",
    "H = 8\n",
    "W = 16\n",
    "\n",
    "x = torch.ones((N, C, H, W))\n",
    "\n",
    "# torch.Size([4, 10, 8, 16])\n",
    "out1 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 3), padding=1)(x)\n",
    "print(out1.shape)\n",
    "\n",
    "# torch.Size([4, 10, 8, 16])\n",
    "out2 = torch.nn.Conv2d(C, C_out, kernel_size=(5, 5), padding=2)(x)\n",
    "print(out2.shape)\n",
    "\n",
    "# torch.Size([4, 10, 8, 16])\n",
    "out3 = torch.nn.Conv2d(C, C_out, kernel_size=(7, 7), padding=3)(x)\n",
    "print(out3.shape)\n",
    "\n",
    "# torch.Size([4, 10, 8, 16])\n",
    "out4 = torch.nn.Conv2d(C, C_out, kernel_size=(9, 9), padding=4)(x)\n",
    "print(out4.shape)\n",
    "\n",
    "# torch.Size([4, 10, 8, 16])\n",
    "out5 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 5), padding=(1, 2))(x)\n",
    "print(out5.shape)\n",
    "\n",
    "# torch.Size([4, 10, 22, 30])\n",
    "out6 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 3), padding=8)(x)\n",
    "print(out6.shape)\n",
    "\n",
    "# torch.Size([4, 10, 7, 15])\n",
    "out7 = torch.nn.Conv2d(C, C_out, kernel_size=(4, 4), padding=1)(x)\n",
    "print(out7.shape)\n",
    "\n",
    "# torch.Size([4, 10, 9, 17])\n",
    "out8 = torch.nn.Conv2d(C, C_out, kernel_size=(2, 2), padding=1)(x)\n",
    "print(out8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом уроке мы детально изучим слои нормализации.\n",
    "\n",
    " \n",
    "\n",
    "Самая популярная версия слоя нормализации - слой нормализации \"по батчу\" (batch-norm слой).\n",
    "\n",
    "Рассмотрим его работу в наиболее простом случае, когда на вход подается батч из одномерных векторов:\n",
    "\n",
    "* На вход подается батч одномерных векторов:\n",
    "\n",
    "<img src='01.PNG'/>\n",
    "\n",
    "где j индекс вектора внутри батча, i - номер компоненты.\n",
    "\n",
    "Для текущего батча:\n",
    "\n",
    "* По каждой компоненте входа вычисляются мат.ожидание и дисперсия:\n",
    "\n",
    "<img src='02.PNG'/>\n",
    "\n",
    "<img src='03.PNG'/>\n",
    "\n",
    "* Вход нормируется по формуле: \n",
    "\n",
    "<img src='04.PNG'/>\n",
    "\n",
    "Эпсилон необходим для случая нулевой дисперсии.\n",
    "\n",
    "* Нормированный вход преобразуется следующим образом:\n",
    "\n",
    "<img src='05.PNG'/>\n",
    "\n",
    "Где Гамма и Бета - обучаемые параметры слоя. Обратите внимание, Гамма и Бета - вектора такой же длины, как инстансы входа.\n",
    "\n",
    "Их можно фиксировать, например, простейший случай - Бета принимается равным нулевому вектору, Гамма - вектору из единиц. \n",
    "\n",
    "Если же взять Гамму равным знаменателю дроби из формулы для Z, а Бету равным мат.ожиданию, то слой вернет входной тензор без изменений. То есть, слой будет эквивалентен тождественной функции.\n",
    "\n",
    "Таким образом, параметры Бета и Гамма позволяют не терять входящию в слой информацию, и одновременно с этим, батч-норм слой нормализует вход. Последнее ускоряет сходимость параметров сети, а в некоторых случаях без нормализации добиться сходимости сети крайне сложно.\n",
    "\n",
    "Итоговая формула преобразования входа: \n",
    "\n",
    "<img src='06.PNG'/>\n",
    "\n",
    "В этом уроке мы будем двигаться по следующему плану:\n",
    "\n",
    "1) Вначале реализуем train-этап батч-нормализации для батча из одномерных векторов с нулевым Бета и единичным Гамма.\n",
    "\n",
    "2) Затем добавим возможность задания параметров Бета и Гамма.\n",
    "\n",
    "3) После этого добавим eval-этап использования слоя.\n",
    "\n",
    "4) И последним шагом по батч-нормализации реализуем train-этап слоя батч-нормализации для батча из многоканальных двумерных тензоров с нулевым Бета и единичным Гамма.\n",
    "\n",
    "5) После батч-нормализации вас ждут шаги по другим видам нормализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
    "\n",
    "* Параметр Бета принимается равным 0.\n",
    "\n",
    "* Параметр Гамма принимается равным 1.\n",
    "\n",
    "* Функция должна корректно работать только на этапе обучения.\n",
    "\n",
    "* Вход имеет размерность число элементов в батче * длина каждого инстанса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, eps):\n",
    "    std, mean = torch.std_mean(input_tensor, dim=0, unbiased=False)\n",
    "    normed_tensor = (input_tensor - mean) / (std**2 + eps)** 0.5 # y_i = z_i\n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
    "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
    "\n",
    "all_correct = True\n",
    "for eps_power in range(10):\n",
    "    eps = np.power(10., -eps_power)\n",
    "    batch_norm.eps = eps\n",
    "    batch_norm_out = batch_norm(input_tensor)\n",
    "    custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
    "\n",
    "    all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
    "    all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного обобщим функцию с предыдущего шага - добавим возможность задавать параметры Бета и Гамма.\n",
    "\n",
    "На данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
    "\n",
    "* Функция должна корректно работать только на этапе обучения.\n",
    "\n",
    "* Вход имеет размерность число элементов в батче * длина каждого инстанса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 7\n",
    "batch_size = 5\n",
    "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
    "    std, mean = torch.std_mean(input_tensor, dim=0, unbiased=False)\n",
    "    normed_tensor = weight * (input_tensor - mean) / (std**2 + eps)** 0.5 + bias\n",
    "    return normed_tensor\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm_out = batch_norm(input_tensor)\n",
    "custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
    "print(torch.allclose(batch_norm_out, custom_batch_norm_out) \\\n",
    "      and batch_norm_out.shape == custom_batch_norm_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавимся еще от одного упрощения - реализуем работу слоя батч-нормализации на этапе предсказания.\n",
    "\n",
    "На этом этапе вместо статистик по батчу будем использовать экспоненциально сглаженные статистики из истории обучения слоя.\n",
    "\n",
    "В данном шаге вам требуется реализовать полноценный класс батч-нормализации без использования стандартной функции, принимающий на вход двумерный тензор. Осторожно, расчёт дисперсии ведётся по смещенной выборке, а расчет скользящего среднего по несмещенной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "input_size = 3\n",
    "batch_size = 5\n",
    "eps = 1e-1\n",
    "\n",
    "\n",
    "class CustomBatchNorm1d:\n",
    "    def __init__(self, weight, bias, eps, momentum):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        input_size = weight.shape[0]\n",
    "        self.running_mean = torch.zeros(input_size)\n",
    "        self.running_var = torch.ones(input_size)\n",
    "        self.predict = False\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        \n",
    "        if self.predict:\n",
    "            normed_tensor = self.weight * (input_tensor \\\n",
    "                                           - self.running_mean) / (self.running_var + self.eps)** 0.5 + self.bias\n",
    "            return normed_tensor\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        var = torch.var(input_tensor, dim=0, unbiased=False)\n",
    "        mean = torch.mean(input_tensor, dim=0)\n",
    "        self.running_mean = (1 - self.momentum) * mean + self.momentum * self.running_mean\n",
    "        self.running_var = (1 - self.momentum) * var * batch_size / (batch_size - 1) \\\n",
    "            + self.momentum * self.running_var\n",
    "        normed_tensor = self.weight * (input_tensor \\\n",
    "                                       - mean) / (var + self.eps)** 0.5 + self.bias\n",
    "        return normed_tensor\n",
    "\n",
    "    def eval(self):\n",
    "        self.predict = True\n",
    "\n",
    "\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.momentum = 0.5\n",
    "\n",
    "custom_batch_norm1d = CustomBatchNorm1d(batch_norm.weight.data,\n",
    "                                        batch_norm.bias.data, eps, batch_norm.momentum)\n",
    "\n",
    "all_correct = True\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "print(all_correct)\n",
    "\n",
    "batch_norm.eval()\n",
    "custom_batch_norm1d.eval()\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли убедиться, реализовать батч-норм слой на этапе предсказания не так просто, поэтому в дальнейших шагах этого урока мы больше не будем требовать реализовать эту часть.\n",
    "\n",
    "Слой батч-нормализации существует для входа любой размерности.\n",
    "\n",
    "В данном шаге рассмотрим его для входа из многоканальных двумерных тензоров, например, изображений.\n",
    "\n",
    "Если вытянуть каждый канал картинки в вектор, то вход будет трехмерным:\n",
    "\n",
    "* количество картинок в батче\n",
    "\n",
    "* число каналов в каждой картинке\n",
    "\n",
    "* число пикселей в картинке\n",
    "\n",
    "Процесс нормализации:\n",
    "\n",
    "1) Вход разбивается на срезы, параллельные синей части. То есть, каждый срез - это все пиксели всех изображений по одному из каналов.\n",
    "\n",
    "2) Для каждого среза считаются мат. ожидание и дисперсия.\n",
    "\n",
    "3) Каждый срез нормализуется независимо.\n",
    " \n",
    "\n",
    "На данном шаге вам предлагается реализовать батч-норм слой для четырехмерного входа (например, батч из многоканальных двумерных картинок) без использования стандартной реализации со следующими упрощениями:\n",
    "\n",
    "* Параметр Бета = 0.\n",
    "\n",
    "* Параметр Гамма = 1.\n",
    "\n",
    "* Функция должна корректно работать только на этапе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 300])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "input_channels = 3\n",
    "batch_size = 3\n",
    "height = 10\n",
    "width = 10\n",
    "\n",
    "batch_norm_2d = nn.BatchNorm2d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, height, width, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_batch_norm2d(input_tensor, eps):\n",
    "    input_channels = input_tensor.shape[1]\n",
    "    input_tensor = input_tensor.transpose(0, 1)\n",
    "    old_shape = input_tensor.shape\n",
    "    input_tensor = torch.reshape(input_tensor, [input_channels, -1])\n",
    "    var = torch.var(input_tensor, dim=1, unbiased=False).unsqueeze(0).transpose(0, 1)\n",
    "    mean = torch.mean(input_tensor, dim=1).unsqueeze(0).transpose(0, 1)\n",
    "    normed_tensor = (input_tensor - mean) / (var + eps)** 0.5\n",
    "    normed_tensor = torch.reshape(normed_tensor, old_shape)\n",
    "    normed_tensor = normed_tensor.transpose(0, 1)\n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "norm_output = batch_norm_2d(input_tensor)\n",
    "custom_output = custom_batch_norm2d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы познакомились поближе с нормализацией \"по батчу\". Для упрощения дальнейшего изложения остановимся на случае трехмерного тензора на входе слоя, если же размерность входа больше трех, то вытянем все размерности кроме первых двух в одну размерность.\n",
    "\n",
    "Бывает нормировка не только по батчу, но и по другим измерениям.\n",
    "\n",
    "Обратите внимание на изображения ниже.\n",
    "\n",
    "<img src=\"norm.PNG\" />\n",
    "\n",
    "Где:\n",
    "\n",
    "* C - число каналов на входе.\n",
    "\n",
    "* N - размер батча.\n",
    "\n",
    "* H, W - размерность по последней (третьей) размерности входа.\n",
    " \n",
    "\n",
    "На изображении можно увидеть следующие виды нормализации:\n",
    "\n",
    "* По батчу.\n",
    "\n",
    "* По каналу.\n",
    "\n",
    "* По инстансу.\n",
    "\n",
    "* По группе.\n",
    "\n",
    "Кроме указанных видов, также существует множество других, выходящих за рамки нашего урока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея, лежащая в основе слоя нормализации \"по каналу\" (Layer norm), что сеть должна быть независимой от контраста исходного изображения.\n",
    "\n",
    "Нормализация \"по каналу\" работает независимо по каждому изображению батча.\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по каналу\" без использования стандартного слоя со следующими упрощениями:\n",
    "\n",
    "* Параметр Бета = 0.\n",
    "\n",
    "* Параметр Гамма = 1.\n",
    "\n",
    "* Требуется реализация только этапа обучения.\n",
    "\n",
    "* Нормализация делается по всем размерностям входа, кроме нулевой.\n",
    "\n",
    "Обратите внимание, что размерность входа на данном шаге не фиксирована.\n",
    "\n",
    "Уточним, что в слое нормализации \"по каналу\" статистики считаются по всем размерностям, кроме нулевой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "def custom_layer_norm(input_tensor, eps):\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    old_shape = input_tensor.shape\n",
    "    input_tensor = torch.reshape(input_tensor, [batch_size, -1])\n",
    "    var = torch.var(input_tensor, dim=1, unbiased=False).unsqueeze(0).transpose(0, 1)\n",
    "    mean = torch.mean(input_tensor, dim=1).unsqueeze(0).transpose(0, 1)\n",
    "    normed_tensor = (input_tensor - mean) / (var + eps) ** 0.5\n",
    "    normed_tensor = torch.reshape(normed_tensor, old_shape)\n",
    "    return normed_tensor\n",
    "\n",
    "all_correct = True\n",
    "for dim_count in range(3, 9):\n",
    "    input_tensor = torch.randn(*list(range(3, dim_count + 2)), dtype=torch.float)\n",
    "    layer_norm = nn.LayerNorm(input_tensor.size()[1:], elementwise_affine=False, eps=eps)\n",
    "\n",
    "    norm_output = layer_norm(input_tensor)\n",
    "    custom_output = custom_layer_norm(input_tensor, eps)\n",
    "\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-2)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация \"по инстансу\" (Instance norm) была изначально разработана для задачи style transfer. Идея, лежащая в основе этого слоя, что сеть должна быть независимой от контраста отдельных каналов исходного изображения.\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по инстансу\" без использования стандартного слоя со следующими упрощениями:\n",
    "\n",
    "* Параметр Бета = 0.\n",
    "\n",
    "* Параметр Гамма = 1.\n",
    "\n",
    "* На вход подается трехмерный тензор (размер батча, число каналов, длина каждого канала инстанса).\n",
    "\n",
    "* Требуется реализация только этапа обучения.\n",
    "\n",
    "В слое нормализации \"по инстансу\" статистики считаются по последней размерности (по каждому входному каналу каждого входного примера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 30])\n",
      "torch.Size([5, 2, 1])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "batch_size = 5\n",
    "input_channels = 2\n",
    "input_length = 30\n",
    "\n",
    "instance_norm = nn.InstanceNorm1d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, input_length, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_instance_norm1d(input_tensor, eps):\n",
    "    var = torch.var(input_tensor, dim=-1, unbiased=False).reshape([input_tensor.shape[0], \n",
    "                                                                   input_tensor.shape[1], 1])\n",
    "    mean = torch.mean(input_tensor, dim=-1).unsqueeze(0).reshape([input_tensor.shape[0], \n",
    "                                                                  input_tensor.shape[1], 1])\n",
    "    normed_tensor = (input_tensor - mean) / (var + eps)** 0.5\n",
    "    return normed_tensor\n",
    "\n",
    "norm_output = instance_norm(input_tensor)\n",
    "custom_output = custom_instance_norm1d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация \"по группе\" (Group norm) - это обобщение нормализации \"по каналу\" и \"по инстансу\".\n",
    "\n",
    "Каналы в изображении не являются полностью независимыми, поэтому возможность использования статистики соседних каналов является преимуществом нормализации \"по группе\" по сравнению с нормализацией \"по инстансу\".\n",
    "\n",
    "В то же время, каналы изображения могут сильно отличатся, поэтому нормализация \"по группе\" является более гибкой, чем нормализация \"по каналу\".\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по группе\" без использования стандартного слоя со следующими упрощениями:\n",
    "\n",
    "* Параметр Бета = 0.\n",
    "\n",
    "* Параметр Гамма = 1.\n",
    "\n",
    "* Требуется реализация только этапа обучения.\n",
    "\n",
    "* На вход подается трехмерный тензор.\n",
    "\n",
    "Также слой принимает на вход число групп.\n",
    "\n",
    "В слое нормализации \"по группе\" статистики считаются очень похоже на нормализацию \"по каналу\", только каналы разбиваются на группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "channel_count = 6\n",
    "eps = 1e-3\n",
    "batch_size = 20\n",
    "input_size = 2\n",
    "\n",
    "input_tensor = torch.randn(batch_size, channel_count, input_size)\n",
    "\n",
    "\n",
    "def custom_group_norm(input_tensor, groups, eps):\n",
    "    # groups - number of groups\n",
    "    batch_size, num_channels = input_tensor.shape[0], input_tensor.shape[1]\n",
    "    old_shape = input_tensor.shape\n",
    "    normed_tensor = torch.zeros(old_shape)\n",
    "    group_size = num_channels // groups\n",
    "    for i in range(groups):\n",
    "        grouped_tensor = input_tensor[:,i*group_size:i*group_size + group_size,:]\n",
    "        grouped_tensor = torch.reshape(grouped_tensor, [batch_size, -1])\n",
    "        var = torch.var(grouped_tensor, dim=1, unbiased=False).unsqueeze(0).transpose(0, 1)\n",
    "        mean = torch.mean(grouped_tensor, dim=1).unsqueeze(0).transpose(0, 1)\n",
    "        result = (grouped_tensor - mean) / (var + eps) ** 0.5\n",
    "        result = torch.reshape(result, normed_tensor[:,i*group_size:i*group_size + group_size,:].shape)\n",
    "        normed_tensor[:,i*group_size:i*group_size + group_size,:] = result\n",
    "    normed_tensor = torch.reshape(normed_tensor, old_shape)\n",
    "    return normed_tensor\n",
    "\n",
    "all_correct = True\n",
    "for groups in [1, 2, 3, 6]:\n",
    "    group_norm = nn.GroupNorm(groups, channel_count, eps=eps, affine=False)\n",
    "    norm_output = group_norm(input_tensor)\n",
    "    custom_output = custom_group_norm(input_tensor, groups, eps)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-3)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим утверждение про затухание градиента на практике. В документации pytorch можно найти следующие функции активации: \n",
    "\n",
    "ELU, Hardtanh, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Hardshrink.\n",
    "\n",
    "Вам предстоит найти активацию, которая приводит к наименьшему затуханию градиента. \n",
    "\n",
    "Для проверки мы сконструируем SimpleNet, которая будет иметь внутри 3 fc-слоя, по 1 нейрону в каждом без bias'ов. Веса этих нейронов мы проинициализируем единицами. На вход в эту сеть будем подавать числа из нормального распределения. Сделаем 200 запусков (NUMBER_OF_EXPERIMENTS) для честного сравнения и посчитаем среднее значение градиента в первом слое. Найдите такую функцию, которая будет давать максимальные значения градиента в первом слое. Все функции активации нужно инициализировать с аргументами по умолчанию (пустыми скобками)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[(0.4682517726859078, ELU(alpha=1.0)), (0.2953543029690627, Hardtanh(min_val=-1.0, max_val=1.0)), (0.3869488418942643, LeakyReLU(negative_slope=0.01)), (0.22678142284043135, LogSigmoid()), (0.35629074271233546, PReLU(num_parameters=1)), (0.39361816600430755, ReLU()), (0.3758565203519538, ReLU6()), (0.39410421015934843, RReLU(lower=0.125, upper=0.3333333333333333)), (0.5512814350565896, SELU()), (0.44961363452719527, CELU(alpha=1.0)), (0.006961749137772131, Sigmoid()), (0.2403151066543069, Softplus(beta=1, threshold=20)), (0.24748901307582855, Softshrink(0.5)), (0.06543945842771791, Softsign()), (0.16056774412398225, Tanh()), (0.024549292927610655, Tanhshrink()), (0.7263762903213501, Hardshrink(0.5))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import *\n",
    "\n",
    "seed = int(input())\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "NUMBER_OF_EXPERIMENTS = 200\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.fc1 = torch.nn.Linear(1, 1, bias=False)  # one neuron without bias\n",
    "        self.fc1.weight.data.fill_(1.)  # init weight with 1\n",
    "        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc2.weight.data.fill_(1.)\n",
    "        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc3.weight.data.fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def get_fc1_grad_abs_value(self):\n",
    "        return torch.abs(self.fc1.weight.grad)\n",
    "\n",
    "def get_fc1_grad_abs_value(net, x):\n",
    "    output = net.forward(x)\n",
    "    output.backward()  # no loss function. Pretending that we want to minimize output\n",
    "                       # In our case output is scalar, so we can calculate backward\n",
    "    fc1_grad = net.get_fc1_grad_abs_value().item()\n",
    "    net.zero_grad()\n",
    "    return fc1_grad\n",
    "\n",
    "activations = [ELU(), Hardtanh(), LeakyReLU(), LogSigmoid(),\n",
    "              PReLU(), ReLU(), ReLU6(), RReLU(), SELU(), CELU(), \n",
    "              Sigmoid(), Softplus(), Softshrink(), Softsign(), \n",
    "              Tanh(), Tanhshrink(), Hardshrink()]\n",
    "\n",
    "grads = []\n",
    "for activation in activations:\n",
    "    net = SimpleNet(activation=activation)\n",
    "\n",
    "    fc1_grads = []\n",
    "    for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n",
    "        fc1_grads.append(get_fc1_grad_abs_value(net, x))\n",
    "\n",
    "    grads.append((np.mean(fc1_grads), activation))\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте решить следующую задачу пользуясь только бумагой и калькулятором:\n",
    "\n",
    "Допустим, у нас есть нейросеть, состоящая из 4-х полносвязных слоев, в каждом из которых по одному нейрону. Для простоты будем считать, что bias'ы у нейронов отсутствуют, а все веса равны 1.\n",
    "\n",
    "После каждого слоя мы поставим активации. В первом случае это будут tanh, во втором - ReLU. Не будем добавлять никакую loss-функцию. Тогда нашу сеть можно будет записать в виде функции:\n",
    "\n",
    "$$ f = a_4(w_4 * a_3(w_3 * a_2(w_2 * a_1(w_1 * x)))) $$\n",
    "\n",
    "Где a - это либо tanh либо ReLU. $w_i$- это одно число.\n",
    "\n",
    "Пусть на вход подали x=100\n",
    "\n",
    "Зная, что $tanh'(x) = 1 - tanh^2(x)$, рассчитайте градиенты весов сети: \n",
    "$[f_{w_1}', f_{w_2}', f_{w_3}', f_{w_4}']$ для случая a=tanh и для случая a=ReLU . Результат округлите до 3-го знака.\n",
    "\n",
    "Правда ли, что для активаций гиперболическим тангенсом, градиенты затухают быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w4 0.436145382444544\n",
      "w3 0.3041246830975466\n",
      "w2 0.16770685876939032\n",
      "w1 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('w4',(1 - np.tanh(np.tanh(np.tanh(np.tanh(100))))**2)*np.tanh(np.tanh(np.tanh(100))))\n",
    "\n",
    "print('w3',(1 - np.tanh(np.tanh(np.tanh(np.tanh(100))))**2)*(1-np.tanh(np.tanh(np.tanh(100)))**2)*np.tanh(np.tanh(100)))\n",
    "\n",
    "print('w2',(1 - np.tanh(np.tanh(np.tanh(np.tanh(100))))**2)*(1-np.tanh(np.tanh(np.tanh(100)))**2)*(1-np.tanh(np.tanh(100))**2)*np.tanh(100))\n",
    "\n",
    "print('w1',(1 - np.tanh(np.tanh(np.tanh(np.tanh(100))))**2)*(1 - np.tanh(np.tanh(np.tanh(100)))**2)*(1 - np.tanh(np.tanh(100))**2)*(1 - np.tanh(100)**2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула ReLU f(x) = max(0, x). \n",
    "\n",
    "Следовательно, производная ReLU для любых x>0 равна 1, т.к. если y=x, то y'(x)=1.  \n",
    "\n",
    "Сложная производная f по w1: \n",
    "ReLU'(ReLU(ReLU(ReLU(100)))) *\n",
    "ReLU'(ReLU(ReLU(100))) *\n",
    "ReLU'(ReLU(100)) *\n",
    "ReLU'(100) * 100\n",
    "\n",
    "Сложная производная f по w2: \n",
    "ReLU'(ReLU(ReLU(ReLU(100)))) *\n",
    "ReLU'(ReLU(ReLU(100))) *\n",
    "ReLU'(ReLU(100)) *\n",
    "ReLU(100)\n",
    "\n",
    "Сложная производная f по w3: \n",
    "ReLU'(ReLU(ReLU(ReLU(100)))) *\n",
    "ReLU'(ReLU(ReLU(100))) *\n",
    "ReLU(ReLU(100))\n",
    "\n",
    "Сложная производная f по w4: \n",
    "ReLU'(ReLU(ReLU(ReLU(100)))) *\n",
    "ReLU(ReLU(ReLU(100)))\n",
    "\n",
    "[0.0, 0.167, 0.304, 0.436], [100.0, 100.0, 100.0, 100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
